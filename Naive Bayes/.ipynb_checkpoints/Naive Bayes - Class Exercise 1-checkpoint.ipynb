{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cccd659b",
   "metadata": {},
   "source": [
    "# Naive Bayes - Class Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3767b345",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c48c074",
   "metadata": {},
   "source": [
    "## Metadata (Data Dictionary)\n",
    "\n",
    "| No.| Variable | Data Type | Description |\n",
    "|----|----------|-----------|-------------|\n",
    "| 1  | customerID | string | ID of the customer |\n",
    "| 2  | gender | string | Gender of the customer |\n",
    "| 3  | SeniorCitizen | string | Whether the customer is a senior citizen (1) or not (0) |\n",
    "| 4  | Partner | string | Whether the customer has a partner |\n",
    "| 5  | Dependents | string | Whether the customer has dependent(s) |\n",
    "| 6  | tenure | int | The duration as a customer (months) |\n",
    "| 7  | PhoneService | string | Whether the customer subscribed to the phone service |\n",
    "| 8  | MultipleLines | string | Whether the customer subscribed to multiple phone services |\n",
    "| 9  | InternetService | string | Type of Internet Service |\n",
    "| 10 | OnlineSecurity | string | Whether the customer subscribed to online security |\n",
    "| 11 | OnlineBackup | string | Whether the customer subscribed to online backup |\n",
    "| 12 | DeviceProtection | string | Whether the customer subscribed to online device protection |\n",
    "| 13 | TechSupport | string | Whether the customer subscribed to technical support |\n",
    "| 14 | StreamingTV | string | Whether the customer subscribed to streaming TV |\n",
    "| 15 | StreamingMovies | string | Whether the customer subscribed to streaming movies |\n",
    "| 16 | Contract | string | Type of Contract |\n",
    "| 17 | PaperlessBilling | string | Whether the customer activated paperless billing |\n",
    "| 18 | PaymentMethod | string | Payment method of the customer |\n",
    "| 19 | MonthlyCharges | float | Monthly charge of the customer |\n",
    "| 20 | TotalCharges | float | Total Charges of the customer |\n",
    "| 21 | Churn | string | Whether the customer left within the last month or not |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a775ff7c",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5f425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d8f4b",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad12564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('_.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b520a94-bfd9-4979-8fc9-9e709cdfbd65",
   "metadata": {},
   "source": [
    "# Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a2bcd-7671-48b2-9b84-304bbdfce9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examinze the missing values\n",
    "\n",
    "columns_with_missing_values = df.columns[df.isnull().any()].to_list()\n",
    "columns_with_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f02db2-a262-42b1-9621-58dee7812a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data types\n",
    "\n",
    "for column in columns_with_missing_values:\n",
    "    print(column, _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6783c298-5b26-4064-934a-8d528a768d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"tenure\" is a numeric variable\n",
    "# the rest are categorical variables\n",
    "# We can fill the missing values by mean for \"tenure\" and by mode for categorical variables\n",
    "\n",
    "for column in columns_with_missing_values:\n",
    "    if column == 'tenure':\n",
    "        df[column] = df[column].fillna(_)\n",
    "    else:\n",
    "        df[column] = df[column].fillna(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fc7886-f69d-44a8-a63e-84fa4f792779",
   "metadata": {},
   "source": [
    "# Handle Categorical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a9f7b3-51f5-4573-ae49-253232fe7dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the columns in object type\n",
    "\n",
    "object_columns = [column for column in df.columns if _]\n",
    "object_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5768e2-9ab3-4611-8c8f-a6e71aed6fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the values in object columns\n",
    "\n",
    "for column in object_columns:\n",
    "    print(column, df[column].unique()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be448db2-81de-4d83-95f3-1ca93e2eefc0",
   "metadata": {},
   "source": [
    "<font color=red><b>Question:</b></font> Why is \"TotalCharges\" in object type? We can take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c89cb35-e9ba-4f8e-b30e-60783082f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Among all object columns, it seems \"TotalCharges\" should be numberic.\n",
    "# It implies that this column contains some non-numeric alphabets\n",
    "\n",
    "df['TotalCharges']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf000b4e-d6c2-48c1-b327-4189e1ed2033",
   "metadata": {},
   "source": [
    "At a glance, it seems to be numeric values. We will need to filter those \"number-like\" values out.\n",
    "\n",
    "How does a \"number-like\" value look like? It should have at most 1 \".\" and all other characters should be digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0ee37-f933-48a8-93ee-a07f3ef70b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a boolean Series with \"number-like\" values to be True\n",
    "mask = df['TotalCharges'].apply(_)\n",
    "\n",
    "# Filter out those \"number-like\" values\n",
    "df['TotalCharges'][~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9b4a58-69c7-479a-8afe-14a39e790470",
   "metadata": {},
   "source": [
    "<font color=red><b>Question:</b></font> What are they? <br>\n",
    "They are missing values in nature. But it has a blank space (\" \") in the cell, so it was not recognized as np.nan.<br>\n",
    "We can change them to np.nan and then replace them by fillna(). However, are they all having 1 black space characters only? We are not abel to visualize. So, we can replace it with empty string (\"\") first and then change them to np.nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e76f60-bd00-4140-930d-78f18ebebf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace ' ' (the blank space) by '' (empty string)\n",
    "df['TotalCharges'] = df['TotalCharges'].map(_)\n",
    "\n",
    "# Replace '' by np.nan\n",
    "df['TotalCharges'] = df['TotalCharges'].replace(_, np.nan)\n",
    "\n",
    "# Convert it to float type\n",
    "df['TotalCharges'] = df['TotalCharges'].astype('float64')\n",
    "\n",
    "# Fill the missing values by mean\n",
    "df['TotalCharges'] = df['TotalCharges'].fillna(df['TotalCharges'].mean())\n",
    "\n",
    "# Check if there are still missing values\n",
    "df['TotalCharges'].isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a474ff7-d55e-4a12-a3b6-a039d66ffc3b",
   "metadata": {},
   "source": [
    "\"False\" means there is no missing value anymore in \"TotalCharges\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b03c21-308c-4372-ba8f-38e63b3d7e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract object columns again\n",
    "object_columns = [column for column in df.columns if df[column].dtype == np.dtype('object')]\n",
    "object_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ace5c87-9322-4463-8cbe-affb3cf5976c",
   "metadata": {},
   "source": [
    "Now, \"TotalCharges\" is not in it anymore. The rest are truly object columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c18680-ced7-47a2-aa35-ed3bb49415fd",
   "metadata": {},
   "source": [
    "# OneHotEncoder\n",
    "We can covert all object columns to multiple binary variables to denote each category.\n",
    "Remember to exclude \"customerID\". You DO NO want to do one-hot encoding on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6590838-ecde-49df-b85f-eff541621e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the original columns and column names\n",
    "\n",
    "raw_columns = df[_]\n",
    "raw_column_names = list(raw_columns.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f929988-1a5c-4198-a71d-0fd8845d8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964b3255-c0f9-4a6d-a9ec-0ac65c42bcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the encoder\n",
    "# drop=\"first\" means it will use the first value as a default value and drop it\n",
    "\n",
    "\n",
    "enc = OneHotEncoder(drop='first')\n",
    "enc.fit(raw_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e39a0b7-9820-4a09-8f13-bb4b7313b274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the encoded columns\n",
    "\n",
    "encoded_columns = enc.transform(raw_columns).toarray()\n",
    "encoded_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ba2dfd-fdfb-4f95-9641-658dcd85110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The encoded columns are stored as np.array\n",
    "# To put them into df, we need to extract the column names\n",
    "\n",
    "encoded_column_names = list(enc.get_feature_names_out())\n",
    "encoded_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22384e32-3e89-4aee-b248-fb9641de282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put them into df\n",
    "\n",
    "df[encoded_column_names] = encoded_columns\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64884a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'Churn_Yes'\n",
    "excluded_features = [label, 'customerID'] + raw_column_names\n",
    "features = [feature for feature in list(df) if feature not in excluded_features]\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69547360",
   "metadata": {},
   "source": [
    "# Manual Way\n",
    "We have learned the theory about naive bayes classifier in the lesson and we practiced doing the calculation manually.<br>\n",
    "Since our memory is still fresh, we can try to perform the manual work by codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a0e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will pick 4 variables for the demonstration\n",
    "\n",
    "feature_set = ['SeniorCitizen', 'Partner_Yes', 'Dependents_Yes', 'PhoneService_Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54588456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview them\n",
    "# They are all binary variables\n",
    "\n",
    "df[_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cebdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the feature columns and the label column\n",
    "\n",
    "df_x = df[_]\n",
    "df_y = df[_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45f6120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the prior probability of y = 1\n",
    "\n",
    "prior_prob = _\n",
    "prior_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all combinations among the feature set\n",
    "\n",
    "combin_df = df[feature_set].drop_duplicates().sort_values(feature_set).reset_index(drop=True)\n",
    "combin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7857db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create 2 DataFrames\n",
    "# One stores the probability of each category of each feature\n",
    "# The other stores the conditional probability\n",
    "\n",
    "prob_df = combin_df.copy()\n",
    "cond_prob_df = combin_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532377c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will demonstrate how to update prob_df and cond_prob_df, using \"SeniorCitizen\" feature\n",
    "\n",
    "feature = 'SeniorCitizen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a72853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will determine the probability of each value for  \"SeniorCitizen\"\n",
    "\n",
    "prob = df[feature].value_counts() / df.shape[0]\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551bdaba-765b-4ef0-8b2c-dd76fb33704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will filter the DataFrame with only positive class\n",
    "# Hence, it will determine the conditional probability of each value for  \"SeniorCitizen\"\n",
    "\n",
    "filtered_df = df[df[label] == 1]\n",
    "cond_prob = filtered_df[feature].value_counts() / filtered_df.shape[0]\n",
    "cond_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1511d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can update it to prob_df (and to cond_prob_df)\n",
    "\n",
    "prob_df[feature] = prob_df[feature].map(lambda x: prob[x])\n",
    "prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c410d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can restart and use a loop to for all features\n",
    "\n",
    "prob_df = combin_df.copy()\n",
    "cond_prob_df = combin_df.copy()\n",
    "\n",
    "for feature in feature_set:\n",
    "    prob = df[feature].value_counts() / df.shape[0]\n",
    "    \n",
    "    filtered_df = df[df[label] == 1]\n",
    "    cond_prob = filtered_df[feature].value_counts() / filtered_df.shape[0]\n",
    "    \n",
    "    prob_df[feature] = prob_df[feature].map(lambda x: prob[x])\n",
    "    cond_prob_df[feature] = cond_prob_df[feature].map(lambda x: cond_prob[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e267e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11071633",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eddf43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can compute the joint probability\n",
    "combin_df['P(B)'] = prob_df._\n",
    "\n",
    "# And the joint conditional probability\n",
    "combin_df['P(B|A=1)'] = cond_prob_df._\n",
    "\n",
    "# Then, we can determine the posterior probability of the target variable\n",
    "combin_df['P(A=1|B)'] = _\n",
    "combin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d6333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a subset df\n",
    "\n",
    "sub_df = df[feature_set+[label]]\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0cc056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a vlookup to get P(A=1|B), which is the posterior probability, from our combin_df\n",
    "\n",
    "sub_df = pd.merge(sub_df, combin_df[feature_set+['P(A=1|B)']], on=feature_set)\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the posterior probability is larger than 0.5, we will predict it as Class 1 (positive)\n",
    "\n",
    "sub_df['yhat'] = (_).astype(int)\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac5a62-796a-484b-b345-73bec302a161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307b6e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(_, _)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f4e776-cd6e-4c49-a87c-296fc8522c07",
   "metadata": {},
   "source": [
    "# Now let's go back to sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a40690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b2d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = CategoricalNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f095dfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yhat = gnb.fit(_, _).predict(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a1f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(_, _)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a040137-c46c-48a7-9bb4-e7021695e96e",
   "metadata": {},
   "source": [
    "See, we get the completely same result with what sklearn does. It shows you are capable of writing the codes behind the imported library!\n",
    "\n",
    "However, it isn't considered as a great result.<br>\n",
    "Both the precision and the recall are not good.<br>\n",
    "We shall try again and include all features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b4fd4e-76e5-4678-8870-8ca2dc7fda98",
   "metadata": {},
   "source": [
    "# Time to get serious. Train Test Split!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646b1d1-049c-46b7-8223-ebd583b0658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63423613-a909-4f8f-8866-328972aadb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520bc40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have set the \"label\" variable and the \"features\" variable previously\n",
    "# Re-run it just in case they are changed\n",
    "\n",
    "label = 'Churn_Yes'\n",
    "excluded_features = [label, 'customerID'] + raw_column_names\n",
    "features = [feature for feature in list(df) if feature not in excluded_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e1df6b-7016-4db4-954e-7eba7b869c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = _\n",
    "train_y = _\n",
    "\n",
    "test_x = _\n",
    "test_y = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14988bb1-f63f-469e-937d-fa6b5eb48447",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CategoricalNB()\n",
    "model.fit(_, _)\n",
    "\n",
    "train_yhat = model.predict(_)\n",
    "test_yhat = model.predict(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dd17e1-d59d-400c-8953-c38615d4a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(_, _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef590e26-dd8c-4a9a-aac8-e57cf3dcc045",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(_, _)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc028ba1-ec2c-4a1f-b754-2e92e1635ac7",
   "metadata": {},
   "source": [
    "Although the precision is still not good (~50%), but the precision has been significantly improved!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dffa86-5b97-4bfc-91ef-e7cfc1364a7e",
   "metadata": {},
   "source": [
    "Subsequently, we will be useing f1-score as the single metric to measure the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85b5322-c5f9-4735-9c79-faa5f74fbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(_, -)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6648b7-d452-42e9-8e64-fa3d525096c6",
   "metadata": {},
   "source": [
    "# Backward Elimination\n",
    "Remember that we did it for linear regression? We can try it here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b0e5e-5711-4f87-b56c-b901c9994ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To begin with, we are dropping no feature\n",
    "features_to_drop = []\n",
    "\n",
    "# Record the best F-score in the iterations\n",
    "best_f1_score = 0\n",
    "\n",
    "# We will loop until we have only 1 variable left\n",
    "# So, if we have n variables, we will loop for n-1 times\n",
    "\n",
    "for i in range(len(features)-1):\n",
    "\n",
    "    # Record the best feature to drop at this iteration\n",
    "    best_feature_to_drop = None\n",
    "    \n",
    "    # We make a for loop to drop a feature each time\n",
    "    for feature_to_drop in features:\n",
    "    \n",
    "        # Skip if feature_to_drop is already in features_to_drop\n",
    "        if feature_to_drop in features_to_drop:\n",
    "            continue\n",
    "\n",
    "        # Re-initiate the training features and the test features\n",
    "        # Drop feature_to_drop and the features in features_to_drop\n",
    "        train_x = train_df[features].drop(features_to_drop, axis=1).drop([feature_to_drop], axis=1)\n",
    "        test_x = test_df[features].drop(features_to_drop, axis=1).drop([feature_to_drop], axis=1)\n",
    "        \n",
    "        model = CategoricalNB()\n",
    "        model.fit(train_x, train_y)\n",
    "        \n",
    "        test_yhat = model.predict(test_x)\n",
    "        f1_score = metrics.f1_score(test_y, test_yhat)\n",
    "\n",
    "        if f1_score >= best_f1_score:\n",
    "            best_f1_score = _\n",
    "            best_feature_to_drop = _\n",
    "\n",
    "    if best_feature_to_drop is None:\n",
    "        print('Iteration {}: Mo improvement. Stop.'.format(i))\n",
    "        break\n",
    "            \n",
    "    features_to_drop.append(best_feature_to_drop)\n",
    "    print('Iteration {}: Dropping {}, F1-score = {}'.format(i, best_feature_to_drop, best_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987467c6-e259-4b29-97c0-f9e8aca234d5",
   "metadata": {},
   "source": [
    "It stopped at the 5th iteration, where dropping more variables would no longer improve the test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2559a9-e45d-4f96-b9d8-80726ed422e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all features except the features in features_to_drop\n",
    "\n",
    "train_x = _\n",
    "test_x = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ba59c-6706-469d-86e8-182ca051166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview it to check\n",
    "\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d47a9-a329-488f-a5ef-3f824cede4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CategoricalNB()\n",
    "model.fit(_, _)\n",
    "test_yhat = model.predict(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ce5b2-d63c-4f8e-8ef0-4f2b5363fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(_, _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852148ca-1571-4fb3-b0e5-e0d05230bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(_, _)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b414096e-8be2-410e-a8ba-2b7f1739606f",
   "metadata": {},
   "source": [
    "# Improving the model by turing alpha (smoothing parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dcd839-ddd5-4a6e-a279-b2e896836286",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = None\n",
    "best_f1_score = 0\n",
    "\n",
    "for i in range(11):\n",
    "    \n",
    "    alpha = 0.5 + i * 0.05\n",
    "    \n",
    "    model = CategoricalNB(alpha=alpha)\n",
    "    model.fit(_, _)\n",
    "    test_yhat = model.predict(test_x)\n",
    "    \n",
    "    f1_score = metrics.f1_score(_, _)\n",
    "\n",
    "    if f1_score > best_f1_score:\n",
    "        best_alpha = _\n",
    "        best_f1_score = _\n",
    "        \n",
    "    print('Alpha: {:.3f}, F1_score: {:.3f}'.format(_, _))\n",
    "\n",
    "print('Best:', best_alpha, best_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bee905-828f-4638-8cb7-cc0c6c55dd3e",
   "metadata": {},
   "source": [
    "Well, there isn't much improvement. But that is how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc738fe-ca3f-4b93-94bd-c25714a8f159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
